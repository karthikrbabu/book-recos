{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Tag Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus of this notebook is to take a look at all the tags that we have present in our data set. From their we clean and normalize the tags, and start to determine what tags are in and out of scope. By the end we try to generalize some larger \"parent\" tags and also try to filter and join the tags with our larger \"Books\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potentially missing imports\n",
    "\n",
    "# !pip install googletrans\n",
    "# !pip install distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#Utility Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Special Imports\n",
    "from nltk.corpus import words\n",
    "from googletrans import Translator\n",
    "import string\n",
    "\n",
    "\n",
    "#ML Imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import sklearn.cluster\n",
    "import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../data/ratings.csv' does not exist: b'../data/ratings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c84dff295089>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'../data/ratings.csv'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'../data/to_read.csv'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'../data/books.csv'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'../data/tags.csv'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../data/ratings.csv' does not exist: b'../data/ratings.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "r = pd.read_csv( '../data/ratings.csv' )\n",
    "tr = pd.read_csv( '../data/to_read.csv' )\n",
    "b = pd.read_csv( '../data/books.csv' )\n",
    "\n",
    "t = pd.read_csv( '../data/tags.csv' )\n",
    "bt = pd.read_csv( '../data/book_tags.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_copy = t.copy()\n",
    "t_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply lower case normalization to all the tags\n",
    "t_copy['tag_name'] = t_copy['tag_name'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initial cleaning \n",
    "for index, row in t_copy.iterrows():\n",
    "    print(str(row['tag_id']) + \" : \" + row['tag_name'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric + Punctuation Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From taking a physical breeze through the book tags at first glance we notice that our tags are in alpha numeric order. From this we start to distinguish sets of tag_ids that are either out of scope or lack proper meaning to our project with the given documentation on the data provided.\n",
    "\n",
    "Before trying any fancy string parsing or regex rules on the words, I start with taking out these rows. That are simply just empty or numerical with punctuation.\n",
    "* Rows [0 - 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in t_copy.iterrows():\n",
    "    if index == 19:\n",
    "        break\n",
    "    print(str(row['tag_id']) + \" : \" + row['tag_name'])\n",
    "    \n",
    "#Get rid of the top 19 rows\n",
    "t_copy = t_copy[19:]    \n",
    "\n",
    "print(t_copy.shape)\n",
    "t_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-English Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an entire chunks of tags that are in Greek and Arabic letters, as a sample we have gone ahead to use Google Translate public Python API to translate a sample of ~870 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Arabic tags\n",
    "# arabic_tags = list(t_copy['tag_name'][33363:34232])\n",
    "# len(arabic_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(arabic_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# translator = Translator()\n",
    "# arabic_translations = translator.translate(arabic_tags, dest='en')\n",
    "# for translation in arabic_translations:\n",
    "#     print(translation.origin, ' -> ', translation.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_copy[33381:33382]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_copy[33381:33382]['tag_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translator = Translator()\n",
    "# translator.translate(str(t_copy[33381:33382]['tag_name'])).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_arabic_book_ids = bt[bt['tag_id'] == 33400][\"goodreads_book_id\"]\n",
    "# list(some_arabic_book_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b[b['goodreads_book_id'].isin(some_arabic_book_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arabic language books, and tags is one example of a speciality in the raw data. For our purposes of cluster analysis, and ultimately prediction based on the books and novels we are strudying bringing new unknown languages into the mix presents a challenge. To be able to make sound predictions and tuning our model to capture the variation of language, phenotics, and culture is a little out of scope for now. Therefore we will be filtering out books and tags that are not of \"English\" origin for our project. Therefore in the cleaning of our tags we will be getting rid of these types of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the physical glance at our data we can simple filter by the row numbers, all non-enligsh tags\n",
    "t_non_english = t_copy[(False == t_copy['tag_name'].str.match(r\"^[a-zA-Z0-9$@$!%*?&#':;^\\-—_,'\\\\/\\\". +()]+$\"))]\n",
    "t_non_english.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in t_non_english.iterrows():\n",
    "    print(str(row[\"tag_id\"]) + \" : \" + str(row[\"tag_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the physical glance at our data we can simple filter by the row numbers, all non-enligsh tags\n",
    "t_english = t_copy[(True == t_copy['tag_name'].str.match(r\"^[a-zA-Z0-9$@$!%*?&#':;^\\-—_,'\\\\/\\\". +()]+$\"))]\n",
    "t_english.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in t_english.iterrows():\n",
    "    print(str(row[\"tag_id\"]) + \" : \" + str(row[\"tag_name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tags that are only english tags\n",
    "t_copy = t_copy[(True == t_copy['tag_name'].str.match(r\"^[a-zA-Z0-9$@$!%*?&#':;^\\-—_,'\\\\/\\\". +()]+$\"))]\n",
    "print(t_copy.shape)\n",
    "t_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint ~~ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial cleaning\n",
    "print(t_copy.shape)\n",
    "for index, row in t_copy.iterrows():\n",
    "    print(str(row['tag_id']) + \" : \" + row['tag_name'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"tag_name\" column can have several generic tags and/or several user defined tags. The user defined tags can be totally custom and specific to that user. Therefore they are not very meaningful to us when trying to deduce high level genres from these tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Books and their Tags\n",
    "bt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Book Tags grouped to get popularity of tags\n",
    "bt_grouped = bt.groupby(by='tag_id').size().reset_index(name='count').sort_values(by='count',ascending=False)\n",
    "bt_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joined to capture count and tag_name\n",
    "t_joined = t_copy.join(bt_grouped.set_index('tag_id'), on='tag_id').sort_values(by=\"count\",ascending=False)\n",
    "t_joined.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom User Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the top 5 popular tags are all rather meaningless to us, therefore we must parse a level deeper to determine genre level tags. Therefore we must take a look to clean out more custom tags, and determine the best high level tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of punctuations from the tags\n",
    "t_joined[\"tag_name\"] = [tag.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) for tag in t_joined[\"tag_name\"]]\n",
    "print(t_joined.shape)\n",
    "t_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate by most popular tags going down\n",
    "print(t_joined.shape)\n",
    "for index, row in t_joined.iterrows():\n",
    "    print(str(row[\"tag_id\"]) + \" : \" + str(row[\"tag_name\"]) + \" : \" + str(row[\"count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Somewhat rudementary but this works! To get rid of a solid chunk of custom tags.\n",
    "user_tags1 = 'to read+|reading+|my books+|wish list+|novel+|series+|^[[:digit:]]*$|i own+|currently+|own+|have+|[^[:alnum:] ]|favorite+|favourite+|'\n",
    "user_tags2 = 'club+|buy+|library+|read+|borrowed+|abandoned+|audio+|ya|e book+|ebook+|kindle+|default+|finish+|maybe+|gave up+|'\n",
    "user_tags3 = 'dnf+|stars+|^(15|16|17|18|19|20)\\d{2}[a-zA-Z]*|century+|grade+'\n",
    "\n",
    "user_tags = user_tags1+user_tags2 + user_tags3\n",
    "\n",
    "del_filter = t_joined[\"tag_name\"].str.contains(user_tags)\n",
    "\n",
    "print(t_joined[del_filter].shape)\n",
    "# Iterate by most popular tags going down\n",
    "for index, row in t_joined[del_filter].iterrows():\n",
    "    print(str(row[\"tag_id\"]) + \" : \" + str(row[\"tag_name\"]) + \" : \" + str(row[\"count\"]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate by most popular tags going down\n",
    "print(t_joined[~del_filter].shape)\n",
    "for index, row in t_joined[~del_filter].iterrows():\n",
    "    print(str(row[\"tag_id\"]) + \" : \" + str(row[\"tag_name\"]) + \" : \" + str(row[\"count\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update value of our tags\n",
    "t_joined = t_joined[~del_filter]\n",
    "\n",
    "print(t_joined.shape)\n",
    "t_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for tags that have been used atleast 10 or more\n",
    "# NOTE - This cuts down the number of \"tag_names\" we have from 27,680 >> 4,450\n",
    "\n",
    "# We are trying  to get high level tag names, therefore doing this kind of trimming doesn't feel too wrong\n",
    "# (might be wrong here)\n",
    "\n",
    "t_joined = t_joined[t_joined[\"count\"] >= 10]\n",
    "t_joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# num_clusters = 5\n",
    "\n",
    "# km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "# %time km.fit(tfidf_matrix)\n",
    "\n",
    "# clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sort_tag_name = t_joined.sort_values(by=\"tag_name\")\n",
    "t_sort_tag_name.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data is in alphabetical order by 'tag_name', creating clusters by iterating through chunks of the data allow us to determine the affinity groups based on the word distanes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sort_tag_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to the large size and getting convergence warnings splitting up the tags into smaller chunks helps the \n",
    "#algorithm\n",
    "tag_split = np.array_split(np.array(t_sort_tag_name['tag_name']),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clusters = []\n",
    "exemplars = []\n",
    "\n",
    "words = tag_split[0] #Replace this line\n",
    "print(\"CHUNK #1 - size : {0}\".format(len(words)))\n",
    "words = np.asarray(words) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "affprop.fit(lev_similarity)\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "\n",
    "    #Add to list for dataframe creation\n",
    "    clusters.append(cluster)\n",
    "    exemplars.append(exemplar)\n",
    "\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "    print()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tag_split[1] #Replace this line\n",
    "print(\"CHUNK #2 - size : {0}\".format(len(words)))\n",
    "words = np.asarray(words) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "affprop.fit(lev_similarity)\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "\n",
    "    #Add to list for dataframe creation\n",
    "    clusters.append(cluster)\n",
    "    exemplars.append(exemplar)\n",
    "\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "    print()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_split[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tag_split[2] #Replace this line\n",
    "print(\"CHUNK #3 - size : {0}\".format(len(words)))\n",
    "words = np.asarray(words) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "affprop.fit(lev_similarity)\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "\n",
    "    #Add to list for dataframe creation\n",
    "    clusters.append(cluster)\n",
    "    exemplars.append(exemplar)\n",
    "\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "    print()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tag_split[3] #Replace this line\n",
    "print(\"CHUNK #4 - size : {0}\".format(len(words)))\n",
    "words = np.asarray(words) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "affprop.fit(lev_similarity)\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "\n",
    "    #Add to list for dataframe creation\n",
    "    clusters.append(cluster)\n",
    "    exemplars.append(exemplar)\n",
    "\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "    print()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tag_split[4] #Replace this line\n",
    "print(\"CHUNK #5 - size : {0}\".format(len(words)))\n",
    "words = np.asarray(words) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "affprop.fit(lev_similarity)\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "\n",
    "    #Add to list for dataframe creation\n",
    "    clusters.append(cluster)\n",
    "    exemplars.append(exemplar)\n",
    "\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "    print()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tag_split[5] #Replace this line\n",
    "print(\"CHUNK #6 - size : {0}\".format(len(words)))\n",
    "words = np.asarray(words) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "affprop.fit(lev_similarity)\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "\n",
    "    #Add to list for dataframe creation\n",
    "    clusters.append(cluster)\n",
    "    exemplars.append(exemplar)\n",
    "\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))\n",
    "    print()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Potential clustered genres\n",
    "genre_clusters = {'Exempler':exemplars,'Cluster':clusters}\n",
    "df_genre_clusters = pd.DataFrame(genre_clusters)\n",
    "df_genre_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress! \n",
    "\n",
    "\n",
    "Essentially we are able to break down the 34,000 plus tags into clustered groups of about 520. From here we can apply another level of processing to try and use the general purpose tags, and from their determine a smaller subset of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
